{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "from Model import *\n",
    "from train import *\n",
    "\n",
    "# load file\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
    "from data_loader import *\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "def ps_labeling(data_loader, model, semi_label, threshold,  dropout,rp = 2):\n",
    "    #semi_label = torch.tensor(semi_label, dtype=torch.long).to(device)\n",
    "    semi_label = torch.zeros(rp, len(semi_label), 60).to(device)\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    for iter_drop in range(rp):\n",
    "\n",
    "        for it, (data, seq_len, label, _, index) in enumerate(data_loader):\n",
    "            if it == 0:\n",
    "                pass\n",
    "            data = data.to(device)\n",
    "\n",
    "            cla_pre_tmp = model(data, seq_len).detach()\n",
    "            cla_pre_tmp = softmax(cla_pre_tmp)\n",
    "            semi_label[iter_drop, index, :] = cla_pre_tmp\n",
    "            break\n",
    "            # index = torch.tensor(index).to(device)\n",
    "            # sub_set, ps_lab = torch.max(cla_pre, 1)\n",
    "            # sub_set = sub_set > 0\n",
    "            # ps_lab = ps_lab +1\n",
    "\n",
    "    np.save('./labels/ps_prob_full_rp%d_drop%.2f.npy'% (rp, dropout), semi_label.cpu().numpy())\n",
    "\n",
    "## training procedure\n",
    "teacher_force = False\n",
    "fix_weight = True\n",
    "fix_state = False\n",
    "few_knn = False\n",
    "max_length = 50\n",
    "pro_tr = 0\n",
    "pro_re = 0\n",
    "phase  = 'RC'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# global variable\n",
    "ProjectFolderName = 'NTUProject'\n",
    "root_path = '/home/ws2/Documents/jingyuan/'\n",
    "train_data = 'NTUtrain_cs_full.h5'\n",
    "test_data = 'NTUtest_cs_full.h5'\n",
    "# hyperparameter\n",
    "feature_length = 75\n",
    "hidden_size = 1024\n",
    "batch_size = 64\n",
    "en_num_layers = 3\n",
    "de_num_layers = 1\n",
    "middle_size = 125\n",
    "cla_num_layers = 1\n",
    "learning_rate = 0.001\n",
    "epoch = 300\n",
    "cla_dim = [60]\n",
    "\n",
    "k = 2 # top k accuracy\n",
    "# for classification\n",
    "dataset_train = MyInterTrainDataset(os.path.join(root_path, ProjectFolderName, train_data), [])\n",
    "semi_label = np.load('./labels/base_semiLabel.npy')#np.zeros(len(dataset_train))\n",
    "indices_train = np.where(semi_label == 0)[0]\n",
    "train_sampler = SubsetRandomSampler(indices_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                           shuffle=False, collate_fn=pad_collate_iter)\n",
    "\n",
    "\n",
    "\n",
    "for drop_out in [0.3]:\n",
    "    model = SemiSeq(feature_length, hidden_size, cla_dim, dropout=drop_out).to(device)\n",
    "    optimizer= optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    model_name = './trained_model/' + 'baselineAP5_layer3_hid1024_epoch193'\n",
    "    model, _ = load_model(model_name, model, optimizer, device)\n",
    "    model.train()\n",
    "    threshold = 0.2\n",
    "    ps_labeling(train_loader, model, semi_label, threshold, dropout=drop_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}